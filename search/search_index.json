{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IceData: Datasets Hub for the IceVision Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Users Forum Join our Devs Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you Why IceData? IceData is a dataset hub for the IceVision Framework It includes community maintained datasets and parsers and has out-of-the-box support for common annotation formats (COCO, VOC, etc.) It provides an overview of each included dataset with a description, an annotation example, and other helpful information It makes end-to-end training straightforward thanks to IceVision's unified API It enables practioners to get moving with object detection technology quickly Datasets Source The Datasets class is designed to simplify loading and parsing a wide range of computer vision datasets. Main Features: Caches data so you don't need to download it over and over Lightweight and fast Transparent and pythonic API Out-of-the-box parsers convert common dataset annotation formats into the unified IceVision Data Format IceData provides several ready-to-use datasets that use both common annotation formats such as COCO and VOC as well as other annotation formats such WheatParser used in the Kaggle Global Wheat Competition Usage Object detection datasets use multiple annotation formats (COCO, VOC, and others). IceVision makes it easy to work across all of them with its easy-to-use and extend parsers. COCO and VOC compatible datasets For COCO or VOC compatible datasets - especially ones that are not include in IceData - it is easiest to use the IceData COCO or VOC parser. Example: Raccoon - a dataset using the VOC parser # Imports from icevision.all import * import icedata # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define the class_map class_map = ClassMap ([ \"raccoon\" ]) # Create a parser for dataset using the predefined icevision VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # Parse the annotations to create the train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Note Notice how we use the predifined parsers.voc() function: parser = parsers.voc( annotations_dir=annotations_dir, images_dir=images_dir, class_map=class_map ) Datasets included in IceData Datasets included in IceData always have their own parser. It can be invoked with icedata. datasetname .parser(...) . Example: The IceData Fridge dataset Please check out the fridge folder for more information on how this dataset is structured. # Imports from icevision.all import * import icedata # Load the Fridge Objects dataset data_dir = icedata . fridge . load () # Get the class_map class_map = icedata . fridge . class_map () # Parse the annotations parser = icedata . fridge . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () # Show images with their boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Note Notice how we use the parser associated with the fridge dataset icedata.fridge.parser() : parser = icedata.fridge.parser(data_dir, class_map) Datasets with a new annotation format Sometimes, you will need to define a new annotation format for you dataset. Additional information can be found in the documentation . In this case, we strongly recommend you following the file structure and naming conventions used in the examples such as the Fridge dataset , or the PETS dataset . Disclaimer Inspired from the excellent HuggingFace Datasets project, icedata is a utility library that downloads and prepares computer vision datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have a license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the its license. If you are a dataset owner and wish to update any of the information in IceData (description, citation, etc.), or do not want your dataset to be included, please get in touch through a GitHub issue . Thanks for your contribution to the ML community! If you are interested in learning more about responsible AI practices, including fairness, please see Google AI's Responsible AI Practices .","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"IceData: Datasets Hub for the IceVision Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Users Forum Join our Devs Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you","title":""},{"location":"#why-icedata","text":"IceData is a dataset hub for the IceVision Framework It includes community maintained datasets and parsers and has out-of-the-box support for common annotation formats (COCO, VOC, etc.) It provides an overview of each included dataset with a description, an annotation example, and other helpful information It makes end-to-end training straightforward thanks to IceVision's unified API It enables practioners to get moving with object detection technology quickly","title":"Why IceData?"},{"location":"#datasets","text":"Source The Datasets class is designed to simplify loading and parsing a wide range of computer vision datasets. Main Features: Caches data so you don't need to download it over and over Lightweight and fast Transparent and pythonic API Out-of-the-box parsers convert common dataset annotation formats into the unified IceVision Data Format IceData provides several ready-to-use datasets that use both common annotation formats such as COCO and VOC as well as other annotation formats such WheatParser used in the Kaggle Global Wheat Competition","title":"Datasets"},{"location":"#usage","text":"Object detection datasets use multiple annotation formats (COCO, VOC, and others). IceVision makes it easy to work across all of them with its easy-to-use and extend parsers.","title":"Usage"},{"location":"#coco-and-voc-compatible-datasets","text":"For COCO or VOC compatible datasets - especially ones that are not include in IceData - it is easiest to use the IceData COCO or VOC parser. Example: Raccoon - a dataset using the VOC parser # Imports from icevision.all import * import icedata # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define the class_map class_map = ClassMap ([ \"raccoon\" ]) # Create a parser for dataset using the predefined icevision VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # Parse the annotations to create the train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Note Notice how we use the predifined parsers.voc() function: parser = parsers.voc( annotations_dir=annotations_dir, images_dir=images_dir, class_map=class_map )","title":"COCO and VOC compatible datasets"},{"location":"#datasets-included-in-icedata","text":"Datasets included in IceData always have their own parser. It can be invoked with icedata. datasetname .parser(...) . Example: The IceData Fridge dataset Please check out the fridge folder for more information on how this dataset is structured. # Imports from icevision.all import * import icedata # Load the Fridge Objects dataset data_dir = icedata . fridge . load () # Get the class_map class_map = icedata . fridge . class_map () # Parse the annotations parser = icedata . fridge . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () # Show images with their boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Note Notice how we use the parser associated with the fridge dataset icedata.fridge.parser() : parser = icedata.fridge.parser(data_dir, class_map)","title":"Datasets included in IceData"},{"location":"#datasets-with-a-new-annotation-format","text":"Sometimes, you will need to define a new annotation format for you dataset. Additional information can be found in the documentation . In this case, we strongly recommend you following the file structure and naming conventions used in the examples such as the Fridge dataset , or the PETS dataset .","title":"Datasets with a new annotation format"},{"location":"#disclaimer","text":"Inspired from the excellent HuggingFace Datasets project, icedata is a utility library that downloads and prepares computer vision datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have a license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the its license. If you are a dataset owner and wish to update any of the information in IceData (description, citation, etc.), or do not want your dataset to be included, please get in touch through a GitHub issue . Thanks for your contribution to the ML community! If you are interested in learning more about responsible AI practices, including fairness, please see Google AI's Responsible AI Practices .","title":"Disclaimer"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"birds/","text":"Name Caltech-UCSD Birds 200 Dataset Description Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos of 200 bird species (mostly North American). For detailed information about the dataset, please see the technical report 1 linked below. Number of categories: 200 Number of images: 6,033 Annotations: Bounding Box, Rough Segmentation, Attributes Annotations Examples Usage A Colab notebook will be added How to load this dataset # Imports from icevision.all import * import icedata # Load the Birds dataset path = icedata . birds . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . birds . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # Birds parser: provided out-of-the-box parser = icedata . birds . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . birds . class_map () model = icedata . birds . trained_models . faster_rcnn_resnet50_fpn () Dataset folders For more information about the dataset, visit the project website Directory Information images/ The images organized in subdirectories based on species. annotations-mat/ Bounding box and rough segmentation annotations. Organized as the images. attributes/ Attribute data from MTurk workers. attributes-yaml/ Contains the same attribute data as in 'attributes/' but stored for each file as a yaml file with the same name as the image file. lists/ classes.txt : list of categories (species) files.txt : list of all image files (including subdirectories) train.txt : list of all images used for training test.txt : list of all images used for testing splits.mat : training/testing splits in MATLAB .mat format Annotations sample It uses MATLAB files. Check out the birds AnnotationParser, BirdMaskFile classes. License Please check out here Relevant Publications Caltech-UCSD Birds 200 Welinder P., Branson S., Mita T., Wah C., Schroff F., Belongie S., Perona, P. California Institute of Technology. CNS-TR-2010-001. 2010 \u21a9","title":"Birds"},{"location":"birds/#name","text":"Caltech-UCSD Birds 200 Dataset","title":"Name"},{"location":"birds/#description","text":"Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos of 200 bird species (mostly North American). For detailed information about the dataset, please see the technical report 1 linked below. Number of categories: 200 Number of images: 6,033 Annotations: Bounding Box, Rough Segmentation, Attributes","title":"Description"},{"location":"birds/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"birds/#usage","text":"A Colab notebook will be added","title":"Usage"},{"location":"birds/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the Birds dataset path = icedata . birds . load_data ()","title":"How to load this dataset"},{"location":"birds/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . birds . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # Birds parser: provided out-of-the-box parser = icedata . birds . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"birds/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . birds . class_map () model = icedata . birds . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"birds/#dataset-folders","text":"For more information about the dataset, visit the project website Directory Information images/ The images organized in subdirectories based on species. annotations-mat/ Bounding box and rough segmentation annotations. Organized as the images. attributes/ Attribute data from MTurk workers. attributes-yaml/ Contains the same attribute data as in 'attributes/' but stored for each file as a yaml file with the same name as the image file. lists/ classes.txt : list of categories (species) files.txt : list of all image files (including subdirectories) train.txt : list of all images used for training test.txt : list of all images used for testing splits.mat : training/testing splits in MATLAB .mat format","title":"Dataset folders"},{"location":"birds/#annotations-sample","text":"It uses MATLAB files. Check out the birds AnnotationParser, BirdMaskFile classes.","title":"Annotations sample"},{"location":"birds/#license","text":"Please check out here","title":"License"},{"location":"birds/#relevant-publications","text":"Caltech-UCSD Birds 200 Welinder P., Branson S., Mita T., Wah C., Schroff F., Belongie S., Perona, P. California Institute of Technology. CNS-TR-2010-001. 2010 \u21a9","title":"Relevant Publications"},{"location":"biwi/","text":"Name BIWI Sample Keypoints (center of face) Description This is the same dataset available in fastai ( here ). It consists of 200 human portraits annotated with (y, x) coordinates of the center of the face. Given its small size, it is ideal for fast experimentations in keypoints detection. Number of categories: 1 (human) Number of images: 200 Annotations: (y, x) coordinates of center of face","title":"Biwi"},{"location":"biwi/#name","text":"BIWI Sample Keypoints (center of face)","title":"Name"},{"location":"biwi/#description","text":"This is the same dataset available in fastai ( here ). It consists of 200 human portraits annotated with (y, x) coordinates of the center of the face. Given its small size, it is ideal for fast experimentations in keypoints detection. Number of categories: 1 (human) Number of images: 200 Annotations: (y, x) coordinates of center of face","title":"Description"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog . [Unreleased] Compatible with icevision 0.10 Changed Added gdrive parameter to load_data Bug fixes Deleted [0.2.0] Compatible with icevision 0.5 Added OCHuman dataset BIWI dataset [0.1.0] IMPORTANT Switched from poetry to setuptools Added Dataset template [0.0.3] Fixes load_data caching [0.0.2] Added Added trained_models module NUM_CLASSES constant to datasets Changed Renamed load to load_data Deleted","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"changelog/#unreleased","text":"Compatible with icevision 0.10","title":"[Unreleased]"},{"location":"changelog/#changed","text":"Added gdrive parameter to load_data Bug fixes","title":"Changed"},{"location":"changelog/#deleted","text":"","title":"Deleted"},{"location":"changelog/#020","text":"Compatible with icevision 0.5","title":"[0.2.0]"},{"location":"changelog/#added","text":"OCHuman dataset BIWI dataset","title":"Added"},{"location":"changelog/#010","text":"","title":"[0.1.0]"},{"location":"changelog/#important","text":"Switched from poetry to setuptools","title":"IMPORTANT"},{"location":"changelog/#added_1","text":"Dataset template","title":"Added"},{"location":"changelog/#003","text":"","title":"[0.0.3]"},{"location":"changelog/#fixes","text":"load_data caching","title":"Fixes"},{"location":"changelog/#002","text":"","title":"[0.0.2]"},{"location":"changelog/#added_2","text":"Added trained_models module NUM_CLASSES constant to datasets","title":"Added"},{"location":"changelog/#changed_1","text":"Renamed load to load_data","title":"Changed"},{"location":"changelog/#deleted_1","text":"","title":"Deleted"},{"location":"changing-the-colors/","text":"Changing the colors If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder Color scheme Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) }) Primary color The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accent color The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Changing the colors"},{"location":"changing-the-colors/#changing-the-colors","text":"If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder","title":"Changing the colors"},{"location":"changing-the-colors/#color-scheme","text":"Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) })","title":"Color scheme"},{"location":"changing-the-colors/#primary-color","text":"The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) })","title":"Primary color"},{"location":"changing-the-colors/#accent-color","text":"The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Accent color"},{"location":"coco/","text":"Name COCO Dataset Description COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoint Annotations Examples Usage A Colab notebook will be added How to load this dataset # Imports from icevision.all import * import icedata # Load the COCO dataset path = icedata . coco . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . coco . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # COCO parser: provided out-of-the-box parser = icedata . coco . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . coco . class_map () model = icedata . coco . trained_models . faster_rcnn_resnet50_fpn () Dataset folders TODO Annotations Structure COCO Bounding box: (x-top left, y-top left, width, height) Pascal VOC Bounding box :(x-top left, y-top left,x-bottom right, y-bottom right) COCO has several annotation types: for object detection, keypoint detection, stuff segmentation, panoptic segmentation, densepose, and image captioning. The annotations are stored using JSON. Please note that the COCO API described on the download page can be used to access and manipulate all anotations. All annotations share the same basic data structure below: { \"info\" : i nf o , \"images\" : [ image ], \"annotations\" : [ a nn o tat io n ], \"licenses\" : [ lice nse ], } i nf o { \"year\" : i nt , \"version\" : s tr , \"description\" : s tr , \"contributor\" : s tr , \"url\" : s tr , \"date_created\" : da tet ime , } image { \"id\" : i nt , \"width\" : i nt , \"height\" : i nt , \"file_name\" : s tr , \"license\" : i nt , \"flickr_url\" : s tr , \"coco_url\" : s tr , \"date_captured\" : da tet ime , } lice nse { \"id\" : i nt , \"name\" : s tr , \"url\" : s tr , } The data structures specific to the object detection annotation types is described below. Object Detection Annotation Each object instance annotation contains a series of fields, including the category id and segmentation mask of the object. The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people). In addition, an enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names. See also the detection task. a nn o tat io n { \"id\" : i nt , \"image_id\" : i nt , \"category_id\" : i nt , \"segmentation\" : RLE or [ polygo n ], \"area\" : fl oa t , \"bbox\" : [ x , y , wid t h , heigh t ], \"iscrowd\" : 0 or 1 , } ca te gories [{ \"id\" : i nt , \"name\" : s tr , \"supercategory\" : s tr , }] License The dataset is available to download for commercial/research purposes under a Creative Commons Attribution-ShareAlike 4.0 International License . The copyright remains with the original owners of the images. Relevant Publications Microsoft COCO: Common Objects in Context Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r","title":"Coco"},{"location":"coco/#name","text":"COCO Dataset","title":"Name"},{"location":"coco/#description","text":"COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoint","title":"Description"},{"location":"coco/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"coco/#usage","text":"A Colab notebook will be added","title":"Usage"},{"location":"coco/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the COCO dataset path = icedata . coco . load_data ()","title":"How to load this dataset"},{"location":"coco/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . coco . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # COCO parser: provided out-of-the-box parser = icedata . coco . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"coco/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . coco . class_map () model = icedata . coco . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"coco/#dataset-folders","text":"TODO","title":"Dataset folders"},{"location":"coco/#annotations-structure","text":"COCO Bounding box: (x-top left, y-top left, width, height) Pascal VOC Bounding box :(x-top left, y-top left,x-bottom right, y-bottom right) COCO has several annotation types: for object detection, keypoint detection, stuff segmentation, panoptic segmentation, densepose, and image captioning. The annotations are stored using JSON. Please note that the COCO API described on the download page can be used to access and manipulate all anotations. All annotations share the same basic data structure below: { \"info\" : i nf o , \"images\" : [ image ], \"annotations\" : [ a nn o tat io n ], \"licenses\" : [ lice nse ], } i nf o { \"year\" : i nt , \"version\" : s tr , \"description\" : s tr , \"contributor\" : s tr , \"url\" : s tr , \"date_created\" : da tet ime , } image { \"id\" : i nt , \"width\" : i nt , \"height\" : i nt , \"file_name\" : s tr , \"license\" : i nt , \"flickr_url\" : s tr , \"coco_url\" : s tr , \"date_captured\" : da tet ime , } lice nse { \"id\" : i nt , \"name\" : s tr , \"url\" : s tr , } The data structures specific to the object detection annotation types is described below.","title":"Annotations Structure"},{"location":"coco/#object-detection-annotation","text":"Each object instance annotation contains a series of fields, including the category id and segmentation mask of the object. The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people). In addition, an enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names. See also the detection task. a nn o tat io n { \"id\" : i nt , \"image_id\" : i nt , \"category_id\" : i nt , \"segmentation\" : RLE or [ polygo n ], \"area\" : fl oa t , \"bbox\" : [ x , y , wid t h , heigh t ], \"iscrowd\" : 0 or 1 , } ca te gories [{ \"id\" : i nt , \"name\" : s tr , \"supercategory\" : s tr , }]","title":"Object Detection Annotation"},{"location":"coco/#license","text":"The dataset is available to download for commercial/research purposes under a Creative Commons Attribution-ShareAlike 4.0 International License . The copyright remains with the original owners of the images.","title":"License"},{"location":"coco/#relevant-publications","text":"Microsoft COCO: Common Objects in Context Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r","title":"Relevant Publications"},{"location":"coco_data/","text":"[source] class_map icedata . datasets . coco . data . class_map ( background = \"background\" )","title":"Data"},{"location":"coco_data/#class_map","text":"icedata . datasets . coco . data . class_map ( background = \"background\" )","title":"class_map"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing IceVision \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icedata repo. \u200b2. Download a copy of your remote username/icedata repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icedata.git Install the requirments. You many use miniconda or conda as well. pip install -r requirements.txt Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with icedata easily. git remote add upstream https://github.com/airctic/icedata.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file.md git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request A. Method 1: Using GitHub CLI Preliminary step (done only once): Install gh by following the instructions in docs . 1. Create a pull request using GitHub CLI # Fill up the PR title and the body gh pr create -B master -b \"enter body of PR here\" -t \"enter title\" 2. Confirm PR was created You can confirm that your PR has been created by running the following command, from the icedata folder: gh pr list You can also check the status of your PR by running: gh pr status More detailed documentation can be found https://cli.github.com/manual/gh_pr . 3. Updating a PR If you want to change your code after a PR has been created, you can do it by sending more commits to the same remote branch. For example: git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> It will automatically show up in the PR on the github page. If these are small changes they can be squashed together by the reviewer at the merge time and appear as a single commit in the repository. B. Method 2: Using Git 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/icedata) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icedata main repo and GitHub will prompt you to create a pull request. 2. Confirm PR was created: Ensure your pr is listed here Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icedata repo. note IceVision has CI checking. It will automatically check your code for build as well. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-icevision","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icedata repo. \u200b2. Download a copy of your remote username/icedata repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icedata.git Install the requirments. You many use miniconda or conda as well. pip install -r requirements.txt","title":"Step 1: Forking and Installing IceVision"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with icedata easily. git remote add upstream https://github.com/airctic/icedata.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file.md git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#a-method-1-using-github-cli","text":"Preliminary step (done only once): Install gh by following the instructions in docs .","title":"A. Method 1: Using GitHub CLI"},{"location":"contributing/#1-create-a-pull-request-using-github-cli","text":"# Fill up the PR title and the body gh pr create -B master -b \"enter body of PR here\" -t \"enter title\"","title":"1. Create a pull request using GitHub CLI"},{"location":"contributing/#2-confirm-pr-was-created","text":"You can confirm that your PR has been created by running the following command, from the icedata folder: gh pr list You can also check the status of your PR by running: gh pr status More detailed documentation can be found https://cli.github.com/manual/gh_pr .","title":"2. Confirm PR was created"},{"location":"contributing/#3-updating-a-pr","text":"If you want to change your code after a PR has been created, you can do it by sending more commits to the same remote branch. For example: git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> It will automatically show up in the PR on the github page. If these are small changes they can be squashed together by the reviewer at the merge time and appear as a single commit in the repository.","title":"3. Updating a PR"},{"location":"contributing/#b-method-2-using-git","text":"","title":"B. Method 2: Using Git"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/icedata) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icedata main repo and GitHub will prompt you to create a pull request.","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created_1","text":"Ensure your pr is listed here Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before>","title":"2. Confirm PR was created:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icedata repo. note IceVision has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parser - Single Object Type The Dataset This tutorial uses the Global Wheat Detection dataset which can be downloaded from Kaggle . The aim is to detect a single object class: the wheat head. Installation and Imports ! wget https : // raw . githubusercontent . com / airctic / icevision / master / install_colab . sh ! bash install_colab . sh from icevision.all import * Uploading the Data from Kaggle You will need your personal Kaggle API token (kaggle.json). If you don't have one yet, go to your Kaggle account. Once inside your Account, under the 'API' portion, click on 'Create New API Token'. from google.colab import files files . upload () # kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. ! mkdir - p ~/. kaggle / ! cp / kaggle . json ~/. kaggle / ! chmod 600 ~/. kaggle / kaggle . json ! pip install -- upgrade -- force - reinstall -- no - deps kaggle ! kaggle competitions download - c global - wheat - detection ! unzip / content / global - wheat - detection . zip - d / content / global - wheat - detection > / dev / null ! ls Saving kaggle.json to kaggle (1).json {'kaggle.json': b'{\"username\":\"marialrodriguez\",\"key\":\"e3f17a764b333ed8cc29ecaf8d2b6b4e\"}'} '=5.1' global-wheat-detection install_colab.sh kaggle.json gdrive global-wheat-detection.zip 'kaggle (1).json' sample_data Change Directories to allow a more direct route % cd global - wheat - detection ! ls # pwd, folders /content/global-wheat-detection '=5.1' kaggle.json train checkpoints models train.csv global-wheat-detection.zip sample_submission.csv install_colab.sh test Quick Exploration of the compiled annotations in the CSV file import pandas as pd df = pd . read_csv ( 'train.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 147790 5e0747034 1024 1024 [134.0, 228.0, 141.0, 71.0] arvalis_2 147791 5e0747034 1024 1024 [430.0, 13.0, 184.0, 79.0] arvalis_2 147792 5e0747034 1024 1024 [875.0, 740.0, 94.0, 61.0] arvalis_2 df . info () df . image_id . nunique () df . source . unique () . tolist () <class 'pandas.core.frame.DataFrame'> RangeIndex: 147793 entries, 0 to 147792 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 image_id 147793 non-null object 1 width 147793 non-null int64 2 height 147793 non-null int64 3 bbox 147793 non-null object 4 source 147793 non-null object dtypes: int64(2), object(3) memory usage: 5.6+ MB 3373 ['usask_1', 'arvalis_1', 'inrae_1', 'ethz_1', 'arvalis_3', 'rres_1', 'arvalis_2'] The dataset has 147,793 entries from 3,373 images. An image may contain more than one object. The filename 'image_id' does not have a '.jpg' extension yet, and will need to be addressed during parsing to correspond with the name for each image in the train folder. The image size is given in 'width' and 'height'. The bounding box coordinates are given in 'bbox' and is formatted as [xmin, ymin, box width, box height]. The 'source' likely pertains to instution source for the image, which is not relevant for this study. Establishing Paths Path ( './' ) . ls () Path ( 'train' ) . ls () # 3,422 images data_dir = Path ( '/content/global-wheat-detection' ) class_map = ClassMap ([ 'wheat' ]) class_map (#10) [Path('=5.1'),Path('checkpoints'),Path('global-wheat-detection.zip'),Path('train.csv'),Path('train'),Path('install_colab.sh'),Path('sample_submission.csv'),Path('models'),Path('kaggle.json'),Path('test')] (#3422) [Path('train/cbbc58a4c.jpg'),Path('train/c2d23766d.jpg'),Path('train/a93b2e119.jpg'),Path('train/d9a2181ed.jpg'),Path('train/b2fbd25ac.jpg'),Path('train/900d273d2.jpg'),Path('train/77222a135.jpg'),Path('train/f7d780303.jpg'),Path('train/5a9a55ae5.jpg'),Path('train/19f4faf0f.jpg')...] <ClassMap: {'background': 0, 'wheat': 1}> Parser The template will help formulate the custom parser. template_record = ObjectDetectionRecord () Parser . generate_template ( template_record ) class WheatHeadParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / 'train.csv' ) self . class_map = ClassMap ([ 'wheat' ]) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . image_id def parse_fields ( self , o , record , is_new ): if is_new : filepath = self . data_dir / 'train' / f ' { o . image_id } .jpg' record . set_filepath ( filepath ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))]) record . detection . add_labels ([ 'wheat' ]) parser = WheatHeadParser ( template_record , data_dir ) train_records , valid_records = parser . parse () show_record ( train_records [ 0 ], figsize = ( 10 , 10 ), display_label = False ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) 0%| | 0/147793 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/3373 [00:00<?, ?it/s] train_records [ 0 ] BaseRecord show_records ( train_records [ 1 : 4 ], ncols = 3 , display_label = False ) common: - Filepath: /content/global-wheat-detection/train/46630486d.jpg - Img: None - Image size ImgSize(width=1024, height=1024) - Record ID: 46630486d detection: - Class Map: <ClassMap: {'background': 0, 'wheat': 1}> - Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] - BBoxes: [<BBox (xmin:955.0, ymin:380.0, xmax:1019.0, ymax:490.0)>, <BBox (xmin:477.0, ymin:532.0, xmax:537.0, ymax:628.0)>, <BBox (xmin:826.0, ymin:950.0, xmax:920.0, ymax:1021.0)>, <BBox (xmin:819.0, ymin:200.0, xmax:908.0, ymax:285.0)>, <BBox (xmin:634.0, ymin:578.0, xmax:750.0, ymax:629.0)>, <BBox (xmin:595.0, ymin:394.0, xmax:671.0, ymax:480.0)>, <BBox (xmin:805.0, ymin:471.0, xmax:866.0, ymax:537.0)>, <BBox (xmin:84.0, ymin:498.0, xmax:116.0, ymax:592.0)>, <BBox (xmin:630.0, ymin:801.0, xmax:719.0, ymax:875.0)>, <BBox (xmin:165.0, ymin:133.0, xmax:226.0, ymax:199.0)>, <BBox (xmin:658.0, ymin:64.0, xmax:690.0, ymax:158.0)>, <BBox (xmin:346.0, ymin:114.0, xmax:400.0, ymax:187.0)>, <BBox (xmin:488.0, ymin:412.0, xmax:569.0, ymax:496.0)>, <BBox (xmin:519.0, ymin:753.0, xmax:579.0, ymax:856.0)>, <BBox (xmin:992.0, ymin:737.0, xmax:1024.0, ymax:821.0)>, <BBox (xmin:233.0, ymin:576.0, xmax:299.0, ymax:621.0)>, <BBox (xmin:9.0, ymin:311.0, xmax:69.0, ymax:364.0)>, <BBox (xmin:471.0, ymin:786.0, xmax:533.0, ymax:857.0)>, <BBox (xmin:921.0, ymin:536.0, xmax:973.0, ymax:576.0)>, <BBox (xmin:311.0, ymin:182.0, xmax:352.0, ymax:260.0)>, <BBox (xmin:734.0, ymin:746.0, xmax:771.0, ymax:820.0)>, <BBox (xmin:610.0, ymin:233.0, xmax:636.0, ymax:314.0)>, <BBox (xmin:425.0, ymin:546.0, xmax:461.0, ymax:615.0)>, <BBox (xmin:368.0, ymin:510.0, xmax:488.0, ymax:543.0)>, <BBox (xmin:0.0, ymin:686.0, xmax:42.0, ymax:733.0)>, <BBox (xmin:393.0, ymin:435.0, xmax:476.0, ymax:473.0)>, <BBox (xmin:694.0, ymin:289.0, xmax:742.0, ymax:327.0)>, <BBox (xmin:363.0, ymin:268.0, xmax:421.0, ymax:309.0)>, <BBox (xmin:166.0, ymin:770.0, xmax:219.0, ymax:838.0)>, <BBox (xmin:136.0, ymin:616.0, xmax:245.0, ymax:666.0)>, <BBox (xmin:774.0, ymin:533.0, xmax:804.0, ymax:598.0)>, <BBox (xmin:315.0, ymin:836.0, xmax:361.0, ymax:875.0)>, <BBox (xmin:445.0, ymin:902.0, xmax:536.0, ymax:940.0)>, <BBox (xmin:439.0, ymin:111.0, xmax:464.0, ymax:189.0)>, <BBox (xmin:161.2, ymin:235.0, xmax:236.8, ymax:269.4)>, <BBox (xmin:312.0, ymin:432.0, xmax:375.0, ymax:493.0)>, <BBox (xmin:522.0, ymin:972.0, xmax:576.0, ymax:998.0)>, <BBox (xmin:378.0, ymin:477.0, xmax:454.0, ymax:515.0)>, <BBox (xmin:398.0, ymin:122.0, xmax:432.0, ymax:199.0)>, <BBox (xmin:577.0, ymin:794.0, xmax:643.0, ymax:853.0)>, <BBox (xmin:109.0, ymin:570.0, xmax:152.0, ymax:610.0)>, <BBox (xmin:118.0, ymin:458.0, xmax:159.0, ymax:495.0)>, <BBox (xmin:357.0, ymin:290.0, xmax:415.0, ymax:341.0)>, <BBox (xmin:143.0, ymin:301.0, xmax:199.0, ymax:322.0)>, <BBox (xmin:234.0, ymin:122.0, xmax:274.0, ymax:188.0)>, <BBox (xmin:235.0, ymin:274.0, xmax:292.0, ymax:326.0)>, <BBox (xmin:845.0, ymin:996.0, xmax:881.0, ymax:1024.0)>, <BBox (xmin:275.0, ymin:50.0, xmax:322.0, ymax:66.0)>, <BBox (xmin:641.0, ymin:884.0, xmax:699.0, ymax:931.0)>, <BBox (xmin:282.0, ymin:395.0, xmax:330.0, ymax:447.0)>, <BBox (xmin:54.0, ymin:740.0, xmax:91.0, ymax:786.0)>, <BBox (xmin:813.0, ymin:898.0, xmax:851.0, ymax:944.0)>, <BBox (xmin:164.0, ymin:534.0, xmax:195.0, ymax:574.0)>, <BBox (xmin:185.0, ymin:835.0, xmax:239.0, ymax:867.0)>, <BBox (xmin:366.0, ymin:508.0, xmax:415.0, ymax:576.0)>, <BBox (xmin:820.0, ymin:243.0, xmax:864.0, ymax:291.0)>, <BBox (xmin:981.0, ymin:802.0, xmax:1020.0, ymax:864.0)>, <BBox (xmin:491.0, ymin:850.0, xmax:541.0, ymax:903.0)>] show_records ( train_records [ 5 : 7 ], display_label = False ) Transforms presize = 512 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = image_size ), tfms . A . Normalize ()]) Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , display_label = False ) Model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 2 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 2 ) Metrics and Learner metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.04786301031708717) learn . fine_tune ( 20 , 4e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 0.643232 0.609152 0.236787 04:38 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='14' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress> 70.00% [14/20 1:06:06<28:19] </div> epoch train_loss valid_loss COCOMetric time 0 0.539485 0.512883 0.329394 04:51 1 0.524518 0.495535 0.346624 04:47 2 0.502157 0.475722 0.369555 04:49 3 0.490112 0.475499 0.365245 04:48 4 0.479012 0.464658 0.375988 04:45 5 0.471368 0.466564 0.378793 04:45 6 0.465504 0.445768 0.400851 04:43 7 0.456183 0.429268 0.409720 04:41 8 0.444008 0.426961 0.418625 04:39 9 0.430694 0.418965 0.421158 04:35 10 0.431581 0.416741 0.429150 04:40 11 0.423635 0.417259 0.428600 04:45 12 0.418421 0.420164 0.428826 04:37 13 0.402397 0.404999 0.436926 04:34 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='85' class='' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress> 100.00% [85/85 00:37<00:00 0.4000] </div> epoch train_loss valid_loss COCOMetric time 0 0.539485 0.512883 0.329394 04:51 1 0.524518 0.495535 0.346624 04:47 2 0.502157 0.475722 0.369555 04:49 3 0.490112 0.475499 0.365245 04:48 4 0.479012 0.464658 0.375988 04:45 5 0.471368 0.466564 0.378793 04:45 6 0.465504 0.445768 0.400851 04:43 7 0.456183 0.429268 0.409720 04:41 8 0.444008 0.426961 0.418625 04:39 9 0.430694 0.418965 0.421158 04:35 10 0.431581 0.416741 0.429150 04:40 11 0.423635 0.417259 0.428600 04:45 12 0.418421 0.420164 0.428826 04:37 13 0.402397 0.404999 0.436926 04:34 14 0.399981 0.406486 0.440906 04:38 15 0.395512 0.400269 0.442585 04:37 16 0.389025 0.400635 0.440688 04:37 17 0.388219 0.399258 0.444396 04:40 18 0.388210 0.398245 0.444733 04:36 19 0.386009 0.397616 0.445394 04:34 ### Visualize Results model_type . show_results ( model , valid_ds ) ![png](images/custom_parser/custom_parser_43_0.png) ### Save the Model from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) fname_model = 'wheat-mmdet_retinanet.pth' torch . save ( model . state_dict (), root_dir / 'models' / fname_model ) Mounted at /content/gdrive ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"Custom Parser"},{"location":"custom_parser/#custom-parser-single-object-type","text":"","title":"Custom Parser - Single Object Type"},{"location":"custom_parser/#the-dataset","text":"This tutorial uses the Global Wheat Detection dataset which can be downloaded from Kaggle . The aim is to detect a single object class: the wheat head.","title":"The Dataset"},{"location":"custom_parser/#installation-and-imports","text":"! wget https : // raw . githubusercontent . com / airctic / icevision / master / install_colab . sh ! bash install_colab . sh from icevision.all import *","title":"Installation and Imports"},{"location":"custom_parser/#uploading-the-data-from-kaggle","text":"You will need your personal Kaggle API token (kaggle.json). If you don't have one yet, go to your Kaggle account. Once inside your Account, under the 'API' portion, click on 'Create New API Token'. from google.colab import files files . upload () # kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. ! mkdir - p ~/. kaggle / ! cp / kaggle . json ~/. kaggle / ! chmod 600 ~/. kaggle / kaggle . json ! pip install -- upgrade -- force - reinstall -- no - deps kaggle ! kaggle competitions download - c global - wheat - detection ! unzip / content / global - wheat - detection . zip - d / content / global - wheat - detection > / dev / null ! ls Saving kaggle.json to kaggle (1).json {'kaggle.json': b'{\"username\":\"marialrodriguez\",\"key\":\"e3f17a764b333ed8cc29ecaf8d2b6b4e\"}'} '=5.1' global-wheat-detection install_colab.sh kaggle.json gdrive global-wheat-detection.zip 'kaggle (1).json' sample_data","title":"Uploading the Data from Kaggle"},{"location":"custom_parser/#change-directories-to-allow-a-more-direct-route","text":"% cd global - wheat - detection ! ls # pwd, folders /content/global-wheat-detection '=5.1' kaggle.json train checkpoints models train.csv global-wheat-detection.zip sample_submission.csv install_colab.sh test","title":"Change Directories to allow a more direct route"},{"location":"custom_parser/#quick-exploration-of-the-compiled-annotations-in-the-csv-file","text":"import pandas as pd df = pd . read_csv ( 'train.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 147790 5e0747034 1024 1024 [134.0, 228.0, 141.0, 71.0] arvalis_2 147791 5e0747034 1024 1024 [430.0, 13.0, 184.0, 79.0] arvalis_2 147792 5e0747034 1024 1024 [875.0, 740.0, 94.0, 61.0] arvalis_2 df . info () df . image_id . nunique () df . source . unique () . tolist () <class 'pandas.core.frame.DataFrame'> RangeIndex: 147793 entries, 0 to 147792 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 image_id 147793 non-null object 1 width 147793 non-null int64 2 height 147793 non-null int64 3 bbox 147793 non-null object 4 source 147793 non-null object dtypes: int64(2), object(3) memory usage: 5.6+ MB 3373 ['usask_1', 'arvalis_1', 'inrae_1', 'ethz_1', 'arvalis_3', 'rres_1', 'arvalis_2'] The dataset has 147,793 entries from 3,373 images. An image may contain more than one object. The filename 'image_id' does not have a '.jpg' extension yet, and will need to be addressed during parsing to correspond with the name for each image in the train folder. The image size is given in 'width' and 'height'. The bounding box coordinates are given in 'bbox' and is formatted as [xmin, ymin, box width, box height]. The 'source' likely pertains to instution source for the image, which is not relevant for this study.","title":"Quick Exploration of the compiled annotations in the CSV file"},{"location":"custom_parser/#establishing-paths","text":"Path ( './' ) . ls () Path ( 'train' ) . ls () # 3,422 images data_dir = Path ( '/content/global-wheat-detection' ) class_map = ClassMap ([ 'wheat' ]) class_map (#10) [Path('=5.1'),Path('checkpoints'),Path('global-wheat-detection.zip'),Path('train.csv'),Path('train'),Path('install_colab.sh'),Path('sample_submission.csv'),Path('models'),Path('kaggle.json'),Path('test')] (#3422) [Path('train/cbbc58a4c.jpg'),Path('train/c2d23766d.jpg'),Path('train/a93b2e119.jpg'),Path('train/d9a2181ed.jpg'),Path('train/b2fbd25ac.jpg'),Path('train/900d273d2.jpg'),Path('train/77222a135.jpg'),Path('train/f7d780303.jpg'),Path('train/5a9a55ae5.jpg'),Path('train/19f4faf0f.jpg')...] <ClassMap: {'background': 0, 'wheat': 1}>","title":"Establishing Paths"},{"location":"custom_parser/#parser","text":"The template will help formulate the custom parser. template_record = ObjectDetectionRecord () Parser . generate_template ( template_record ) class WheatHeadParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / 'train.csv' ) self . class_map = ClassMap ([ 'wheat' ]) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . image_id def parse_fields ( self , o , record , is_new ): if is_new : filepath = self . data_dir / 'train' / f ' { o . image_id } .jpg' record . set_filepath ( filepath ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))]) record . detection . add_labels ([ 'wheat' ]) parser = WheatHeadParser ( template_record , data_dir ) train_records , valid_records = parser . parse () show_record ( train_records [ 0 ], figsize = ( 10 , 10 ), display_label = False ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) 0%| | 0/147793 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/3373 [00:00<?, ?it/s] train_records [ 0 ] BaseRecord show_records ( train_records [ 1 : 4 ], ncols = 3 , display_label = False ) common: - Filepath: /content/global-wheat-detection/train/46630486d.jpg - Img: None - Image size ImgSize(width=1024, height=1024) - Record ID: 46630486d detection: - Class Map: <ClassMap: {'background': 0, 'wheat': 1}> - Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] - BBoxes: [<BBox (xmin:955.0, ymin:380.0, xmax:1019.0, ymax:490.0)>, <BBox (xmin:477.0, ymin:532.0, xmax:537.0, ymax:628.0)>, <BBox (xmin:826.0, ymin:950.0, xmax:920.0, ymax:1021.0)>, <BBox (xmin:819.0, ymin:200.0, xmax:908.0, ymax:285.0)>, <BBox (xmin:634.0, ymin:578.0, xmax:750.0, ymax:629.0)>, <BBox (xmin:595.0, ymin:394.0, xmax:671.0, ymax:480.0)>, <BBox (xmin:805.0, ymin:471.0, xmax:866.0, ymax:537.0)>, <BBox (xmin:84.0, ymin:498.0, xmax:116.0, ymax:592.0)>, <BBox (xmin:630.0, ymin:801.0, xmax:719.0, ymax:875.0)>, <BBox (xmin:165.0, ymin:133.0, xmax:226.0, ymax:199.0)>, <BBox (xmin:658.0, ymin:64.0, xmax:690.0, ymax:158.0)>, <BBox (xmin:346.0, ymin:114.0, xmax:400.0, ymax:187.0)>, <BBox (xmin:488.0, ymin:412.0, xmax:569.0, ymax:496.0)>, <BBox (xmin:519.0, ymin:753.0, xmax:579.0, ymax:856.0)>, <BBox (xmin:992.0, ymin:737.0, xmax:1024.0, ymax:821.0)>, <BBox (xmin:233.0, ymin:576.0, xmax:299.0, ymax:621.0)>, <BBox (xmin:9.0, ymin:311.0, xmax:69.0, ymax:364.0)>, <BBox (xmin:471.0, ymin:786.0, xmax:533.0, ymax:857.0)>, <BBox (xmin:921.0, ymin:536.0, xmax:973.0, ymax:576.0)>, <BBox (xmin:311.0, ymin:182.0, xmax:352.0, ymax:260.0)>, <BBox (xmin:734.0, ymin:746.0, xmax:771.0, ymax:820.0)>, <BBox (xmin:610.0, ymin:233.0, xmax:636.0, ymax:314.0)>, <BBox (xmin:425.0, ymin:546.0, xmax:461.0, ymax:615.0)>, <BBox (xmin:368.0, ymin:510.0, xmax:488.0, ymax:543.0)>, <BBox (xmin:0.0, ymin:686.0, xmax:42.0, ymax:733.0)>, <BBox (xmin:393.0, ymin:435.0, xmax:476.0, ymax:473.0)>, <BBox (xmin:694.0, ymin:289.0, xmax:742.0, ymax:327.0)>, <BBox (xmin:363.0, ymin:268.0, xmax:421.0, ymax:309.0)>, <BBox (xmin:166.0, ymin:770.0, xmax:219.0, ymax:838.0)>, <BBox (xmin:136.0, ymin:616.0, xmax:245.0, ymax:666.0)>, <BBox (xmin:774.0, ymin:533.0, xmax:804.0, ymax:598.0)>, <BBox (xmin:315.0, ymin:836.0, xmax:361.0, ymax:875.0)>, <BBox (xmin:445.0, ymin:902.0, xmax:536.0, ymax:940.0)>, <BBox (xmin:439.0, ymin:111.0, xmax:464.0, ymax:189.0)>, <BBox (xmin:161.2, ymin:235.0, xmax:236.8, ymax:269.4)>, <BBox (xmin:312.0, ymin:432.0, xmax:375.0, ymax:493.0)>, <BBox (xmin:522.0, ymin:972.0, xmax:576.0, ymax:998.0)>, <BBox (xmin:378.0, ymin:477.0, xmax:454.0, ymax:515.0)>, <BBox (xmin:398.0, ymin:122.0, xmax:432.0, ymax:199.0)>, <BBox (xmin:577.0, ymin:794.0, xmax:643.0, ymax:853.0)>, <BBox (xmin:109.0, ymin:570.0, xmax:152.0, ymax:610.0)>, <BBox (xmin:118.0, ymin:458.0, xmax:159.0, ymax:495.0)>, <BBox (xmin:357.0, ymin:290.0, xmax:415.0, ymax:341.0)>, <BBox (xmin:143.0, ymin:301.0, xmax:199.0, ymax:322.0)>, <BBox (xmin:234.0, ymin:122.0, xmax:274.0, ymax:188.0)>, <BBox (xmin:235.0, ymin:274.0, xmax:292.0, ymax:326.0)>, <BBox (xmin:845.0, ymin:996.0, xmax:881.0, ymax:1024.0)>, <BBox (xmin:275.0, ymin:50.0, xmax:322.0, ymax:66.0)>, <BBox (xmin:641.0, ymin:884.0, xmax:699.0, ymax:931.0)>, <BBox (xmin:282.0, ymin:395.0, xmax:330.0, ymax:447.0)>, <BBox (xmin:54.0, ymin:740.0, xmax:91.0, ymax:786.0)>, <BBox (xmin:813.0, ymin:898.0, xmax:851.0, ymax:944.0)>, <BBox (xmin:164.0, ymin:534.0, xmax:195.0, ymax:574.0)>, <BBox (xmin:185.0, ymin:835.0, xmax:239.0, ymax:867.0)>, <BBox (xmin:366.0, ymin:508.0, xmax:415.0, ymax:576.0)>, <BBox (xmin:820.0, ymin:243.0, xmax:864.0, ymax:291.0)>, <BBox (xmin:981.0, ymin:802.0, xmax:1020.0, ymax:864.0)>, <BBox (xmin:491.0, ymin:850.0, xmax:541.0, ymax:903.0)>] show_records ( train_records [ 5 : 7 ], display_label = False )","title":"Parser"},{"location":"custom_parser/#transforms","text":"presize = 512 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = image_size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"custom_parser/#train-and-validation-dataset-objects","text":"train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , display_label = False )","title":"Train and Validation Dataset Objects"},{"location":"custom_parser/#model","text":"selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Model"},{"location":"custom_parser/#dataloaders","text":"train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 2 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 2 )","title":"DataLoaders"},{"location":"custom_parser/#metrics-and-learner","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.04786301031708717) learn . fine_tune ( 20 , 4e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 0.643232 0.609152 0.236787 04:38 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='14' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress> 70.00% [14/20 1:06:06<28:19] </div> epoch train_loss valid_loss COCOMetric time 0 0.539485 0.512883 0.329394 04:51 1 0.524518 0.495535 0.346624 04:47 2 0.502157 0.475722 0.369555 04:49 3 0.490112 0.475499 0.365245 04:48 4 0.479012 0.464658 0.375988 04:45 5 0.471368 0.466564 0.378793 04:45 6 0.465504 0.445768 0.400851 04:43 7 0.456183 0.429268 0.409720 04:41 8 0.444008 0.426961 0.418625 04:39 9 0.430694 0.418965 0.421158 04:35 10 0.431581 0.416741 0.429150 04:40 11 0.423635 0.417259 0.428600 04:45 12 0.418421 0.420164 0.428826 04:37 13 0.402397 0.404999 0.436926 04:34 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='85' class='' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress> 100.00% [85/85 00:37<00:00 0.4000] </div> epoch train_loss valid_loss COCOMetric time 0 0.539485 0.512883 0.329394 04:51 1 0.524518 0.495535 0.346624 04:47 2 0.502157 0.475722 0.369555 04:49 3 0.490112 0.475499 0.365245 04:48 4 0.479012 0.464658 0.375988 04:45 5 0.471368 0.466564 0.378793 04:45 6 0.465504 0.445768 0.400851 04:43 7 0.456183 0.429268 0.409720 04:41 8 0.444008 0.426961 0.418625 04:39 9 0.430694 0.418965 0.421158 04:35 10 0.431581 0.416741 0.429150 04:40 11 0.423635 0.417259 0.428600 04:45 12 0.418421 0.420164 0.428826 04:37 13 0.402397 0.404999 0.436926 04:34 14 0.399981 0.406486 0.440906 04:38 15 0.395512 0.400269 0.442585 04:37 16 0.389025 0.400635 0.440688 04:37 17 0.388219 0.399258 0.444396 04:40 18 0.388210 0.398245 0.444733 04:36 19 0.386009 0.397616 0.445394 04:34 ### Visualize Results model_type . show_results ( model , valid_ds ) ![png](images/custom_parser/custom_parser_43_0.png) ### Save the Model from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) fname_model = 'wheat-mmdet_retinanet.pth' torch . save ( model . state_dict (), root_dir / 'models' / fname_model ) Mounted at /content/gdrive ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"Metrics and Learner"},{"location":"datasets_tools/","text":"Datasets Tools The goal is to gather some useful resources that will help both beginners and advanced users creating and/or processing their datasets. Datasets repositories VisualData.io : A search engine that references hundreds of datasets. Roboflow Datasets : Roboflow hosts free public computer vision datasets in many popular formats (including CreateML JSON, COCO JSON, Pascal VOC XML, YOLO v3, and Tensorflow TFRecords). Open Images : Huge dataset containing more than 1,700,000 images with related bounding boxes, and 600 classes. Datasets creation tools OIDv4 ToolKit : Open Images Dataset v4 Toolkit. Object Detection download any of the 600 classes of the dataset individually, taking care of creating the related bounding boxes for each downloaded image download multiple classes at the same time creating separated folder and bounding boxes for each of them download multiple classes and creating a common folder for all of them with a unique annotation file of each image download a single class or multiple classes with the desired attributes use the practical visualizer to inspect the donwloaded classes Image Classification download any classes in a common labeled folder exploit tens of possible commands to select only the desired images Useful tools such annotations tools Remo : a web-based application to organize, annotate and visualize Computer Vision datasets. Remo runs on Windows, Linux, Mac or directly in Google Colab Notebooks. It can also be served on a private server for team collaboration, or embedded in Jupyter Notebooks. VoTT (Visual Object Tagging Tool) : annotation and labeling tool for image and video assets. VoTT can be installed as a native application or run from source. VoTT is also available as a stand-alone Web application and can be used in any modern Web browser.","title":"Datasets Tools"},{"location":"datasets_tools/#datasets-tools","text":"The goal is to gather some useful resources that will help both beginners and advanced users creating and/or processing their datasets.","title":"Datasets Tools"},{"location":"datasets_tools/#datasets-repositories","text":"VisualData.io : A search engine that references hundreds of datasets. Roboflow Datasets : Roboflow hosts free public computer vision datasets in many popular formats (including CreateML JSON, COCO JSON, Pascal VOC XML, YOLO v3, and Tensorflow TFRecords). Open Images : Huge dataset containing more than 1,700,000 images with related bounding boxes, and 600 classes.","title":"Datasets repositories"},{"location":"datasets_tools/#datasets-creation-tools","text":"OIDv4 ToolKit : Open Images Dataset v4 Toolkit.","title":"Datasets creation tools"},{"location":"datasets_tools/#object-detection","text":"download any of the 600 classes of the dataset individually, taking care of creating the related bounding boxes for each downloaded image download multiple classes at the same time creating separated folder and bounding boxes for each of them download multiple classes and creating a common folder for all of them with a unique annotation file of each image download a single class or multiple classes with the desired attributes use the practical visualizer to inspect the donwloaded classes","title":"Object Detection"},{"location":"datasets_tools/#image-classification","text":"download any classes in a common labeled folder exploit tens of possible commands to select only the desired images","title":"Image Classification"},{"location":"datasets_tools/#useful-tools-such-annotations-tools","text":"Remo : a web-based application to organize, annotate and visualize Computer Vision datasets. Remo runs on Windows, Linux, Mac or directly in Google Colab Notebooks. It can also be served on a private server for team collaboration, or embedded in Jupyter Notebooks. VoTT (Visual Object Tagging Tool) : annotation and labeling tool for image and video assets. VoTT can be installed as a native application or run from source. VoTT is also available as a stand-alone Web application and can be used in any modern Web browser.","title":"Useful tools such annotations tools"},{"location":"fridge/","text":"Name Fridge Objects Dataset Description Fridge Objects is a toy dataset which consists of 134 images with 4 classes of beverage container {can, carton, milk bottle, water bottle} pictures taken on different backgrounds. Annotations Examples Usage Example showing how to use this dataset How to load this dataset # Imports from icevision.all import * import icedata # Load the Fridge Objects dataset path = icedata . fridge . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . fridge . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # Fridge parser: provided out-of-the-box parser = icedata . fridge . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . fridge . class_map () model = icedata . fridge . trained_models . faster_rcnn_resnet50_fpn () Dataset folders Annotations sample <annotation> <folder> images </folder> <filename> 2.jpg </filename> <path> ../images/2.jpg </path> <source> <database> Unknown </database> </source> <size> <width> 499 </width> <height> 666 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> milk_bottle </name> <pose> Unspecified </pose> <truncated> 0 </truncated> <difficult> 0 </difficult> <bndbox> <xmin> 247 </xmin> <ymin> 192 </ymin> <xmax> 355 </xmax> <ymax> 493 </ymax> </bndbox> </object> </annotation> License Unknown Relevant Publications Unknown","title":"Fridge"},{"location":"fridge/#name","text":"Fridge Objects Dataset","title":"Name"},{"location":"fridge/#description","text":"Fridge Objects is a toy dataset which consists of 134 images with 4 classes of beverage container {can, carton, milk bottle, water bottle} pictures taken on different backgrounds.","title":"Description"},{"location":"fridge/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"fridge/#usage","text":"Example showing how to use this dataset","title":"Usage"},{"location":"fridge/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the Fridge Objects dataset path = icedata . fridge . load_data ()","title":"How to load this dataset"},{"location":"fridge/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . fridge . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # Fridge parser: provided out-of-the-box parser = icedata . fridge . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"fridge/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . fridge . class_map () model = icedata . fridge . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"fridge/#dataset-folders","text":"","title":"Dataset folders"},{"location":"fridge/#annotations-sample","text":"<annotation> <folder> images </folder> <filename> 2.jpg </filename> <path> ../images/2.jpg </path> <source> <database> Unknown </database> </source> <size> <width> 499 </width> <height> 666 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> milk_bottle </name> <pose> Unspecified </pose> <truncated> 0 </truncated> <difficult> 0 </difficult> <bndbox> <xmin> 247 </xmin> <ymin> 192 </ymin> <xmax> 355 </xmax> <ymax> 493 </ymax> </bndbox> </object> </annotation>","title":"Annotations sample"},{"location":"fridge/#license","text":"Unknown","title":"License"},{"location":"fridge/#relevant-publications","text":"Unknown","title":"Relevant Publications"},{"location":"fridge_data/","text":"[source] class_map icedata . datasets . fridge . data . class_map ( background = \"background\" ) [source] load_data icedata . datasets . fridge . data . load_data ( force_download = False )","title":"Data"},{"location":"fridge_data/#class_map","text":"icedata . datasets . fridge . data . class_map ( background = \"background\" ) [source]","title":"class_map"},{"location":"fridge_data/#load_data","text":"icedata . datasets . fridge . data . load_data ( force_download = False )","title":"load_data"},{"location":"fridge_parser/","text":"[source] parser icedata . datasets . fridge . parser . parser ( data_dir )","title":"Parsers"},{"location":"fridge_parser/#parser","text":"icedata . datasets . fridge . parser . parser ( data_dir )","title":"parser"},{"location":"how-to/","text":"Where can I get some help? If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum . How to install icedata? To install the icedata package as well as all its dependencies, choose one of the 2 options: Installing the icedata lastest version pip install git+git://github.com/airctic/icedata.git#egg = icedata --upgrade Install the icedata lastest version from Pypi repository: pip install icedata For more options, and more in-depth explanation on how to install icedata, please check out our Installation Guide How to save trained weights in Google Colab? In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by icedata Check out the Train a Dataset Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icedata/models/fridge/fridge_tf_efficientdet_lite0.pth' ) How to load pretrained weights? In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) How to contribute? We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How to"},{"location":"how-to/#where-can-i-get-some-help","text":"If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum .","title":"Where can I get some help?"},{"location":"how-to/#how-to-install-icedata","text":"To install the icedata package as well as all its dependencies, choose one of the 2 options: Installing the icedata lastest version pip install git+git://github.com/airctic/icedata.git#egg = icedata --upgrade Install the icedata lastest version from Pypi repository: pip install icedata For more options, and more in-depth explanation on how to install icedata, please check out our Installation Guide","title":"How to install icedata?"},{"location":"how-to/#how-to-save-trained-weights-in-google-colab","text":"In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by icedata Check out the Train a Dataset Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icedata/models/fridge/fridge_tf_efficientdet_lite0.pth' )","title":"How to save trained weights in Google Colab?"},{"location":"how-to/#how-to-load-pretrained-weights","text":"In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict )","title":"How to load pretrained weights?"},{"location":"how-to/#how-to-contribute","text":"We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How to contribute?"},{"location":"install/","text":"Important We currently only support Linux/MacOS installations A- Installation using pip Option 1: Installing from pypi repository [Stable Version] To install icedata package together with all dependencies: $ pip install icedata Option 2: Installing an editable package locally [For Developers] Note This method is used by developers who are usually either: actively contributing to icedata project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icedata latest version. Clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icedata.git $ cd icedata $ pip install -e \".[dev]\" ### **Option 3:** Installing a non-editable package from GitHub **[Recommended for Active Users]** To install the icedata package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icedata latest version (from the `master` branch) $ pip install git+git://github.com/airctic/icedata.git --upgrade ## B- Installation using conda Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called **ice** $ conda create -n ice python = 3 .8 anaconda $ conda activate ice $ pip install git+git://github.com/airctic/icedata.git#egg = icedata !!! info \"Note\" You can check out the following blog post: [3 ways to pip install a package ](https://ai-fast-track.github.io/blog/python/2020/03/17/how-to-pip-install-package.html) for more a detailed explantion on how to choose the most convenient installation option for you.","title":"Installation"},{"location":"install/#a-installation-using-pip","text":"","title":"A- Installation using pip"},{"location":"install/#option-1-installing-from-pypi-repository-stable-version","text":"To install icedata package together with all dependencies: $ pip install icedata","title":"Option 1: Installing from pypi repository [Stable Version]"},{"location":"install/#option-2-installing-an-editable-package-locally-for-developers","text":"Note This method is used by developers who are usually either: actively contributing to icedata project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icedata latest version. Clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icedata.git $ cd icedata $ pip install -e \".[dev]\" ### **Option 3:** Installing a non-editable package from GitHub **[Recommended for Active Users]** To install the icedata package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icedata latest version (from the `master` branch) $ pip install git+git://github.com/airctic/icedata.git --upgrade ## B- Installation using conda Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called **ice** $ conda create -n ice python = 3 .8 anaconda $ conda activate ice $ pip install git+git://github.com/airctic/icedata.git#egg = icedata !!! info \"Note\" You can check out the following blog post: [3 ways to pip install a package ](https://ai-fast-track.github.io/blog/python/2020/03/17/how-to-pip-install-package.html) for more a detailed explantion on how to choose the most convenient installation option for you.","title":"Option 2: Installing an editable package locally [For Developers]"},{"location":"ochuman/","text":"Name OCHuman Dataset Description 13360 annotated human instances (19 keypoints) within 5081 images. Important : Please keep in mind to manually download AND unzip the dataset from here . In order for the IceData parser to work on this data, you will need the ochuman.json annotations file and the images directory containing the actual images. You will get both unzipping the folder obtained here . Check out the parser in action at ochuman.ipynb .","title":"Ochuman"},{"location":"ochuman/#name","text":"OCHuman Dataset","title":"Name"},{"location":"ochuman/#description","text":"13360 annotated human instances (19 keypoints) within 5081 images. Important : Please keep in mind to manually download AND unzip the dataset from here . In order for the IceData parser to work on this data, you will need the ochuman.json annotations file and the images directory containing the actual images. You will get both unzipping the folder obtained here . Check out the parser in action at ochuman.ipynb .","title":"Description"},{"location":"parsing_cheatsheet/","text":"Parsing Cheatsheet The goal is to create a resource that will help both beginners and advanced users to easily create their parsers by providing some frequently used code snippets and best practices Useful example Custom Parser Use template generator The IceVision template generator helps you to generate all the methods that you need to implement based on the parsers mixins. The first step is to create a class that inherits from these smaller building blocks called mixins: Mixins This is just an example, choose the mixins that are relevant to your use case. class WheatParser(parsers.FasterRCNN, parsers.FilepathMixin, parsers.SizeMixin): pass We use a method called generate_template that will print out all the necessary methods we have to implement. WheatParser.generate_template() Output: def __iter__(self) -> Any: def height(self, o) -> int: def width(self, o) -> int: def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: def record_id(self, o) -> Hashable: If, for example, all the images are .jpg and located in the data_dir folder, the image_paths attribute will be set as follow: def __init__(self, data_dir): self.image_paths = get_files(data_dir, extensions=[\".jpg\"]) Files code snippets Let's suppose we have the follwoing fname variable: fname = Path(\"PennFudanPed/PNGImages/FudanPed00002.png\") Some useful methods are listed below: fname PennFudanPed/PNGImages/FudanPed00002.png fname.exists() True fname.with_suffix('.txt') PennFudanPed/PNGImages/FudanPed00002.txt fname.stem FudanPed00002 Parsing code snippets Read a CSV file using pandas import pandas as pd df = pd.read_csv(\"path/to/csv/file\") df.head() # or df.sample() Example of parsing bboxes attributes defined as an array, and stored in a string: bbox = \"[834.0, 222.0, 56.0, 36.0]\"` xywh = np.fromstring(bbox[1:-1], sep=\",\") print(xywh) Output: array([834., 222., 56., 36.]) Example of parsing bboxes attributes defined as a string with a blank separator label = \"2 0.527267 0.702972 0.945466 0.467218\" xywh = np.fromstring(label, sep=\" \")[1:] print(xywh) Output: array([0.527267, 0.702972, 0.945466, 0.467218]) Masks Let's assume we have the following dictionnary entries to parse in order to create the corresponding masks. Check out the full dictionnary annotations.json \"annotations\": [ { \"segmentation\": [ [457.3, 258.92, 458.38, 276.22, 467.03, 289.19, 473.51, 305.41, 483.24, 334.59,...] ], \"area\": 43522.80595, \"iscrowd\": 0, \"image_id\": 343934, \"bbox\": [ 175.14, 175.68, 321.08, 240 ], \"category_id\": 4, \"id\": 150977 }, { \"segmentation\": [ [507.9, 413.08, ...] ... } ... ] We can implement the abstract method masks() , defined in the abstract class MasksMixin that is inhereted by the MaskRCNN class (see parsers documentation ), as follow: class COCOAnnotationParser(MaskRCNN, COCOBBoxParser): def masks(self, o) -> List[MaskArray]: seg = o[\"segmentation\"] if o[\"iscrowd\"]: return [RLE.from_coco(seg[\"counts\"])] else: return [Polygon(seg)]","title":"Parsing Cheatsheet"},{"location":"parsing_cheatsheet/#parsing-cheatsheet","text":"The goal is to create a resource that will help both beginners and advanced users to easily create their parsers by providing some frequently used code snippets and best practices","title":"Parsing Cheatsheet"},{"location":"parsing_cheatsheet/#useful-example","text":"Custom Parser","title":"Useful example"},{"location":"parsing_cheatsheet/#use-template-generator","text":"The IceVision template generator helps you to generate all the methods that you need to implement based on the parsers mixins. The first step is to create a class that inherits from these smaller building blocks called mixins: Mixins This is just an example, choose the mixins that are relevant to your use case. class WheatParser(parsers.FasterRCNN, parsers.FilepathMixin, parsers.SizeMixin): pass We use a method called generate_template that will print out all the necessary methods we have to implement. WheatParser.generate_template() Output: def __iter__(self) -> Any: def height(self, o) -> int: def width(self, o) -> int: def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: def record_id(self, o) -> Hashable: If, for example, all the images are .jpg and located in the data_dir folder, the image_paths attribute will be set as follow: def __init__(self, data_dir): self.image_paths = get_files(data_dir, extensions=[\".jpg\"])","title":"Use template generator"},{"location":"parsing_cheatsheet/#files-code-snippets","text":"Let's suppose we have the follwoing fname variable: fname = Path(\"PennFudanPed/PNGImages/FudanPed00002.png\") Some useful methods are listed below: fname PennFudanPed/PNGImages/FudanPed00002.png fname.exists() True fname.with_suffix('.txt') PennFudanPed/PNGImages/FudanPed00002.txt fname.stem FudanPed00002","title":"Files code snippets"},{"location":"parsing_cheatsheet/#parsing-code-snippets","text":"","title":"Parsing code snippets"},{"location":"parsing_cheatsheet/#read-a-csv-file-using-pandas","text":"import pandas as pd df = pd.read_csv(\"path/to/csv/file\") df.head() # or df.sample()","title":"Read a CSV file using pandas"},{"location":"parsing_cheatsheet/#example-of-parsing-bboxes-attributes-defined-as-an-array-and-stored-in-a-string","text":"bbox = \"[834.0, 222.0, 56.0, 36.0]\"` xywh = np.fromstring(bbox[1:-1], sep=\",\") print(xywh) Output: array([834., 222., 56., 36.])","title":"Example of parsing bboxes attributes defined as an array, and stored in a string:"},{"location":"parsing_cheatsheet/#example-of-parsing-bboxes-attributes-defined-as-a-string-with-a-blank-separator","text":"label = \"2 0.527267 0.702972 0.945466 0.467218\" xywh = np.fromstring(label, sep=\" \")[1:] print(xywh) Output: array([0.527267, 0.702972, 0.945466, 0.467218])","title":"Example of parsing bboxes attributes defined as a string with a blank separator"},{"location":"parsing_cheatsheet/#masks","text":"Let's assume we have the following dictionnary entries to parse in order to create the corresponding masks. Check out the full dictionnary annotations.json \"annotations\": [ { \"segmentation\": [ [457.3, 258.92, 458.38, 276.22, 467.03, 289.19, 473.51, 305.41, 483.24, 334.59,...] ], \"area\": 43522.80595, \"iscrowd\": 0, \"image_id\": 343934, \"bbox\": [ 175.14, 175.68, 321.08, 240 ], \"category_id\": 4, \"id\": 150977 }, { \"segmentation\": [ [507.9, 413.08, ...] ... } ... ] We can implement the abstract method masks() , defined in the abstract class MasksMixin that is inhereted by the MaskRCNN class (see parsers documentation ), as follow: class COCOAnnotationParser(MaskRCNN, COCOBBoxParser): def masks(self, o) -> List[MaskArray]: seg = o[\"segmentation\"] if o[\"iscrowd\"]: return [RLE.from_coco(seg[\"counts\"])] else: return [Polygon(seg)]","title":"Masks"},{"location":"pennfudan/","text":"Name Penn-Fudan Database for Pedestrian Detection and Segmentation Description This is an image database containing images that are used for pedestrian detection in the experiments reported in 1 . The images are taken from scenes around campus and urban street. The objects we are interested in these images are pedestrians. Each image will have at least one pedestrian in it. The heights of labeled pedestrians in this database fall into [180,390] pixels. All labeled pedestrians are straight up. There are 170 images with 345 labeled pedestrians, among which 96 images are taken from around University of Pennsylvania, and other 74 are taken from around Fudan University. Annotations Examples Usage Example showing how to use this dataset How to load this dataset # Imports from icevision.all import * import icedata # Load the PennFudan dataset path = icedata . pennfudan . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pennfudan . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # PennFudan parser: provided out-of-the-box parser = icedata . pennfudan . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . pennfudan . class_map () model = icedata . pennfudan . trained_models . faster_rcnn_resnet50_fpn () Dataset folders Annotations sample # Compatible with PASCAL Annotation Version 1.00 Image filename : \"PennFudanPed/PNGImages/FudanPed00001.png\" Image size ( X x Y x C ) : 559 x 536 x 3 Database : \"The Penn-Fudan-Pedestrian Database\" Objects with ground truth : 2 { \"PASpersonWalking\" \"PASpersonWalking\" } # Note there may be some objects not included in the ground truth list for they are severe-occluded # or have very small size. # Top left pixel co-ordinates : (1, 1) # Details for pedestrian 1 (\"PASpersonWalking\") Original label for object 1 \"PASpersonWalking\" : \"PennFudanPed\" Bounding box for object 1 \"PASpersonWalking\" ( Xmin, Ymin ) - ( Xmax, Ymax ) : ( 160 , 182 ) - ( 302 , 431 ) Pixel mask for object 1 \"PASpersonWalking\" : \"PennFudanPed/PedMasks/FudanPed00001_mask.png\" # Details for pedestrian 2 (\"PASpersonWalking\") Original label for object 2 \"PASpersonWalking\" : \"PennFudanPed\" Bounding box for object 2 \"PASpersonWalking\" ( Xmin, Ymin ) - ( Xmax, Ymax ) : ( 420 , 171 ) - ( 535 , 486 ) Pixel mask for object 2 \"PASpersonWalking\" : \"PennFudanPed/PedMasks/FudanPed00001_mask.png\" License Please, check out here Relevant Publications Object Detection Combining Recognition and Segmentation. Liming Wang, Jianbo Shi, Gang Song, I-fan Shen. ACCV 2007 pp 189-199 \u21a9","title":"Pennfudan"},{"location":"pennfudan/#name","text":"Penn-Fudan Database for Pedestrian Detection and Segmentation","title":"Name"},{"location":"pennfudan/#description","text":"This is an image database containing images that are used for pedestrian detection in the experiments reported in 1 . The images are taken from scenes around campus and urban street. The objects we are interested in these images are pedestrians. Each image will have at least one pedestrian in it. The heights of labeled pedestrians in this database fall into [180,390] pixels. All labeled pedestrians are straight up. There are 170 images with 345 labeled pedestrians, among which 96 images are taken from around University of Pennsylvania, and other 74 are taken from around Fudan University.","title":"Description"},{"location":"pennfudan/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"pennfudan/#usage","text":"Example showing how to use this dataset","title":"Usage"},{"location":"pennfudan/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the PennFudan dataset path = icedata . pennfudan . load_data ()","title":"How to load this dataset"},{"location":"pennfudan/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pennfudan . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # PennFudan parser: provided out-of-the-box parser = icedata . pennfudan . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"pennfudan/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . pennfudan . class_map () model = icedata . pennfudan . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"pennfudan/#dataset-folders","text":"","title":"Dataset folders"},{"location":"pennfudan/#annotations-sample","text":"# Compatible with PASCAL Annotation Version 1.00 Image filename : \"PennFudanPed/PNGImages/FudanPed00001.png\" Image size ( X x Y x C ) : 559 x 536 x 3 Database : \"The Penn-Fudan-Pedestrian Database\" Objects with ground truth : 2 { \"PASpersonWalking\" \"PASpersonWalking\" } # Note there may be some objects not included in the ground truth list for they are severe-occluded # or have very small size. # Top left pixel co-ordinates : (1, 1) # Details for pedestrian 1 (\"PASpersonWalking\") Original label for object 1 \"PASpersonWalking\" : \"PennFudanPed\" Bounding box for object 1 \"PASpersonWalking\" ( Xmin, Ymin ) - ( Xmax, Ymax ) : ( 160 , 182 ) - ( 302 , 431 ) Pixel mask for object 1 \"PASpersonWalking\" : \"PennFudanPed/PedMasks/FudanPed00001_mask.png\" # Details for pedestrian 2 (\"PASpersonWalking\") Original label for object 2 \"PASpersonWalking\" : \"PennFudanPed\" Bounding box for object 2 \"PASpersonWalking\" ( Xmin, Ymin ) - ( Xmax, Ymax ) : ( 420 , 171 ) - ( 535 , 486 ) Pixel mask for object 2 \"PASpersonWalking\" : \"PennFudanPed/PedMasks/FudanPed00001_mask.png\"","title":"Annotations sample"},{"location":"pennfudan/#license","text":"Please, check out here","title":"License"},{"location":"pennfudan/#relevant-publications","text":"Object Detection Combining Recognition and Segmentation. Liming Wang, Jianbo Shi, Gang Song, I-fan Shen. ACCV 2007 pp 189-199 \u21a9","title":"Relevant Publications"},{"location":"pets/","text":"Name The Oxford-IIIT Pet Dataset Description Pet dataset has 37 classes roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation. Annotations Examples Usage Example showing how to use this dataset How to load this dataset # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . pets . class_map () model = icedata . pets . trained_models . faster_rcnn_resnet50_fpn () Dataset folders Annotations sample <annotation> <folder> OXIIIT </folder> <filename> Abyssinian_1.jpg </filename> <source> <database> OXFORD-IIIT Pet Dataset </database> <annotation> OXIIIT </annotation> <image> flickr </image> </source> <size> <width> 600 </width> <height> 400 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> cat </name> <pose> Frontal </pose> <truncated> 0 </truncated> <occluded> 0 </occluded> <bndbox> <xmin> 333 </xmin> <ymin> 72 </ymin> <xmax> 425 </xmax> <ymax> 158 </ymax> </bndbox> <difficult> 0 </difficult> </object> </annotation> License The dataset is available to download for commercial/research purposes under a Creative Commons Attribution-ShareAlike 4.0 International License . The copyright remains with the original owners of the images. Relevant Publications Cats and Dogs O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar IEEE Conference on Computer Vision and Pattern Recognition, 2012","title":"Pets"},{"location":"pets/#name","text":"The Oxford-IIIT Pet Dataset","title":"Name"},{"location":"pets/#description","text":"Pet dataset has 37 classes roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation.","title":"Description"},{"location":"pets/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"pets/#usage","text":"Example showing how to use this dataset","title":"Usage"},{"location":"pets/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data ()","title":"How to load this dataset"},{"location":"pets/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"pets/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . pets . class_map () model = icedata . pets . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"pets/#dataset-folders","text":"","title":"Dataset folders"},{"location":"pets/#annotations-sample","text":"<annotation> <folder> OXIIIT </folder> <filename> Abyssinian_1.jpg </filename> <source> <database> OXFORD-IIIT Pet Dataset </database> <annotation> OXIIIT </annotation> <image> flickr </image> </source> <size> <width> 600 </width> <height> 400 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> cat </name> <pose> Frontal </pose> <truncated> 0 </truncated> <occluded> 0 </occluded> <bndbox> <xmin> 333 </xmin> <ymin> 72 </ymin> <xmax> 425 </xmax> <ymax> 158 </ymax> </bndbox> <difficult> 0 </difficult> </object> </annotation>","title":"Annotations sample"},{"location":"pets/#license","text":"The dataset is available to download for commercial/research purposes under a Creative Commons Attribution-ShareAlike 4.0 International License . The copyright remains with the original owners of the images.","title":"License"},{"location":"pets/#relevant-publications","text":"Cats and Dogs O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar IEEE Conference on Computer Vision and Pattern Recognition, 2012","title":"Relevant Publications"},{"location":"voc/","text":"Name PASCAL VOC 2012 Dataset Description This dataset contains the data from the PASCAL Visual Object Classes Challenge 2012, a.k.a. VOC2012, corresponding to the Classification and Detection competitions. A total of 11540 images are included in this dataset, where each image contains a set of objects, out of 20 different classes, making a total of 27450 annotated objects. 20 classes: Person: person Animal: bird, cat, cow, dog, horse, sheep Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor Annotations Examples Usage Example showing a dataset using the voc parser How to load this dataset # Imports from icevision.all import * import icedata # Load the Pascal VOC dataset path = icedata . voc . load_data () How to parse this dataset # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . voc . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # VOC parser: provided out-of-the-box parser = parsers . VOCBBoxParser ( annotations_dir = path / 'Annotations' , images_dir = path / 'JPEGImages' , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) How to load the pretrained weights of this dataset class_map = icedata . voc . class_map () model = icedata . voc . trained_models . faster_rcnn_resnet50_fpn () Dataset folders Annotations sample <annotation> <folder> VOC2012 </folder> <filename> 2007_000027.jpg </filename> <source> <database> The VOC2007 Database </database> <annotation> PASCAL VOC2007 </annotation> <image> flickr </image> </source> <size> <width> 486 </width> <height> 500 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> person </name> <pose> Unspecified </pose> <truncated> 0 </truncated> <difficult> 0 </difficult> <bndbox> <xmin> 174 </xmin> <ymin> 101 </ymin> <xmax> 349 </xmax> <ymax> 351 </ymax> </bndbox> <part> <name> head </name> <bndbox> <xmin> 169 </xmin> <ymin> 104 </ymin> <xmax> 209 </xmax> <ymax> 146 </ymax> </bndbox> </part> <part> <name> hand </name> <bndbox> <xmin> 278 </xmin> <ymin> 210 </ymin> <xmax> 297 </xmax> <ymax> 233 </ymax> </bndbox> </part> <part> <name> foot </name> <bndbox> <xmin> 273 </xmin> <ymin> 333 </ymin> <xmax> 297 </xmax> <ymax> 354 </ymax> </bndbox> </part> <part> <name> foot </name> <bndbox> <xmin> 319 </xmin> <ymin> 307 </ymin> <xmax> 340 </xmax> <ymax> 326 </ymax> </bndbox> </part> </object> </annotation> License Please check out here Relevant Publications The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.","title":"Voc"},{"location":"voc/#name","text":"PASCAL VOC 2012 Dataset","title":"Name"},{"location":"voc/#description","text":"This dataset contains the data from the PASCAL Visual Object Classes Challenge 2012, a.k.a. VOC2012, corresponding to the Classification and Detection competitions. A total of 11540 images are included in this dataset, where each image contains a set of objects, out of 20 different classes, making a total of 27450 annotated objects. 20 classes: Person: person Animal: bird, cat, cow, dog, horse, sheep Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor","title":"Description"},{"location":"voc/#annotations-examples","text":"","title":"Annotations Examples"},{"location":"voc/#usage","text":"Example showing a dataset using the voc parser","title":"Usage"},{"location":"voc/#how-to-load-this-dataset","text":"# Imports from icevision.all import * import icedata # Load the Pascal VOC dataset path = icedata . voc . load_data ()","title":"How to load this dataset"},{"location":"voc/#how-to-parse-this-dataset","text":"# Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . voc . class_map () # Randomly split our data into train/valid data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # VOC parser: provided out-of-the-box parser = parsers . VOCBBoxParser ( annotations_dir = path / 'Annotations' , images_dir = path / 'JPEGImages' , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True )","title":"How to parse this dataset"},{"location":"voc/#how-to-load-the-pretrained-weights-of-this-dataset","text":"class_map = icedata . voc . class_map () model = icedata . voc . trained_models . faster_rcnn_resnet50_fpn ()","title":"How to load the pretrained weights of this dataset"},{"location":"voc/#dataset-folders","text":"","title":"Dataset folders"},{"location":"voc/#annotations-sample","text":"<annotation> <folder> VOC2012 </folder> <filename> 2007_000027.jpg </filename> <source> <database> The VOC2007 Database </database> <annotation> PASCAL VOC2007 </annotation> <image> flickr </image> </source> <size> <width> 486 </width> <height> 500 </height> <depth> 3 </depth> </size> <segmented> 0 </segmented> <object> <name> person </name> <pose> Unspecified </pose> <truncated> 0 </truncated> <difficult> 0 </difficult> <bndbox> <xmin> 174 </xmin> <ymin> 101 </ymin> <xmax> 349 </xmax> <ymax> 351 </ymax> </bndbox> <part> <name> head </name> <bndbox> <xmin> 169 </xmin> <ymin> 104 </ymin> <xmax> 209 </xmax> <ymax> 146 </ymax> </bndbox> </part> <part> <name> hand </name> <bndbox> <xmin> 278 </xmin> <ymin> 210 </ymin> <xmax> 297 </xmax> <ymax> 233 </ymax> </bndbox> </part> <part> <name> foot </name> <bndbox> <xmin> 273 </xmin> <ymin> 333 </ymin> <xmax> 297 </xmax> <ymax> 354 </ymax> </bndbox> </part> <part> <name> foot </name> <bndbox> <xmin> 319 </xmin> <ymin> 307 </ymin> <xmax> 340 </xmax> <ymax> 326 </ymax> </bndbox> </part> </object> </annotation>","title":"Annotations sample"},{"location":"voc/#license","text":"Please check out here","title":"License"},{"location":"voc/#relevant-publications","text":"The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.","title":"Relevant Publications"}]}